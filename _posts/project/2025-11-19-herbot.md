---
layout: post
title: Herbot
subtitle: Autonomous Herb Managing Robot
tags: [robot, raspberry-pi, edge-tpu, computer-vision]
author: solgyu lee
category: project
mathjax: false
mermaid: true
thumbnail-img: "/assets/img/Herbot/Herbot.jpeg"
---

Herbot is an autonomous herb management robot I developed as part of the "AI Embedded Programming" course. The system goes beyond simple watering and lighting control‚Äîit automatically identifies and removes diseased leaves through vision-based AI.

<video width="70%" controls>
  <source src="/assets/img/Herbot/Herbot-video.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

---

## What Makes Herbot Different

| Feature | Traditional Plant Systems | Herbot |
|---------|--------------------------|--------|
| Plant Care | Manual watering & lighting | Sensor-based automatic control |
| Disease Management | Human inspection required | AI-powered automatic detection & removal |
| Coverage | Fixed position | 360¬∞ full coverage via rotating platform |
| Intelligence | Simple timer-based | Edge TPU accelerated vision models |

---

## System Architecture

<div class="mermaid">
flowchart TB
    subgraph UI["üñ•Ô∏è User Interface"]
        WEB[Web-based Control]
    end

    subgraph CORE["üß† Central Control"]
        RPI[Raspberry Pi 4]
        TPU[Coral Edge TPU]
    end

    subgraph LEAF["üåø Leaf Removal Module"]
        STEP[Stepper Motor + Belt Rail]
        CAR[Carriage]
        LIN[Linear Actuator]
        CAM[Pi Camera]
        GRIP[Gripper/Cutter]
    end

    subgraph POT["üîÑ Pot Rotation Module"]
        DC[DC Motor]
        PLAT[Rotating Platform]
        HERB[Herb Pot]
    end

    subgraph ENV["üíß Environment Module"]
        LED[LED Lighting]
        HUM[Humidity Sensor]
        PUMP[Water Pump]
    end

    WEB --> RPI
    RPI <--> TPU
    RPI --> STEP
    STEP --> CAR
    CAR --> LIN
    CAR --> CAM
    LIN --> GRIP
    RPI --> DC
    DC --> PLAT
    PLAT --> HERB
    RPI --> LED
    RPI --> HUM
    RPI --> PUMP
</div>

---

## Components

| Category | Components |
|----------|------------|
| **Computing** | Raspberry Pi 4, Coral USB Accelerator |
| **Motion** | NEMA 17 Stepper, Linear Actuator, DC Motor, Servo |
| **Sensing** | Pi Camera |
| **Drivers** | TMC2209 (Stepper), 2x TB6612 (DC/Linear) |
| **Power** | 12V Supply, DC-DC Converter (12V‚Üí6V) |

---

## Mechanical Design

The robot's manipulation system consists of four motors working in coordination:

- **Servo Motor**: Controls the gripper mechanism for cutting herbs
- **NEMA 17 Stepper**: Drives Z-axis movement via belt-driven conveyor system
- **Linear Actuator**: Provides X-axis (radial) motion for reach
- **DC Motor**: Rotates the plant pot, enabling 360¬∞ coverage with a single-rail structure

This design allows full spatial access to all parts of the plant without requiring a complex multi-axis gantry system.

---

## Power Distribution

<div class="mermaid">
flowchart TB
    PWR["üîå 12V 10A Power Supply"]

    PWR --> TMC["TMC2209<br/>Stepper Driver"]
    PWR --> DCDC["DC-DC Converter<br/>12V ‚Üí 6V"]
    PWR --> TB1["TB6612<br/>DC Motor Driver"]

    DCDC --> RPI["Raspberry Pi 4<br/>5V"]
    DCDC --> TB2["TB6612<br/>Linear Actuator"]

    TMC --> M1["NEMA 17<br/>Stepper Motor"]
    TB1 --> M2["DC Motor<br/>Pot Rotation"]
    TB2 --> M3["Linear Actuator<br/>X-axis Motion"]
</div>

The system runs on three voltage levels: 12V for the stepper motor, 6V for the linear actuator and DC motor, and 5V for the Raspberry Pi. Power distribution is managed through WAGO terminal blocks for clean, modular connections.

---

## Web Interface

<img src="/assets/img/Herbot/Herbot-web.PNG" alt="Herbot Web Interface" style="width:70%;">

The web interface provides:
- **Live camera feed** for real-time plant monitoring
- **Scan control** to trigger systematic multi-angle photography
- **Manual overrides** for individual motor control
- **System logs** for debugging and operation history

<img src="/assets/img/Herbot/Herbot-web-log.PNG" alt="Herbot Web Logs" style="width:70%;">

---

## AI Vision Pipeline

<div class="mermaid">
flowchart TB
    CAM["üì∑ Camera Capture<br/>640√ó480"]
    PRE["üîÑ Preprocessing<br/>224√ó224, RGB"]

    subgraph MODELS["AI Models"]
        HERB["üåø Herbify Model<br/>91 herb species"]
        PLANT["üî¨ PlantDoc Model<br/>28 health conditions"]
    end

    subgraph DECISION["Decision Logic"]
        HEALTHY["‚úÖ Healthy<br/>Pass"]
        DISEASED["‚ùå Diseased<br/>Remove"]
    end

    CAM --> PRE
    PRE --> HERB
    PRE --> PLANT
    HERB --> LOG["üìù Species Logged"]
    PLANT --> CHECK{"Confidence<br/>> 60%?"}
    CHECK -->|Yes + Disease Keyword| DISEASED
    CHECK -->|No or Healthy| HEALTHY
</div>

### Dual Model System

| Model | Purpose | Classes | Inference Time |
|-------|---------|---------|----------------|
| **Herbify** | Species identification | 91 herb types | 10-15ms |
| **PlantDoc** | Health assessment | 28 conditions | 5-12ms |

### Disease Detection Logic

<div class="mermaid">
flowchart LR
    INPUT["PlantDoc<br/>Prediction"]

    INPUT --> CONF{"Confidence<br/>Level?"}

    CONF -->|"< 40%"| IGN["üö´ Ignore<br/>Background"]
    CONF -->|"40-60%"| PASS["‚úÖ Pass<br/>Low confidence"]
    CONF -->|"> 60%"| KEY{"Disease<br/>Keyword?"}

    KEY -->|"blight, spot,<br/>rust, scab..."| REM["‚ùå Remove"]
    KEY -->|"ends with 'leaf'<br/>no keywords"| SAFE["‚úÖ Healthy"]
</div>

---

## Edge TPU Optimization

To achieve real-time performance on embedded hardware:

| Optimization | Effect |
|--------------|--------|
| INT8 Quantization | Reduced model size, faster inference |
| Edge TPU Compilation | Hardware-accelerated inference |
| No normalization | Direct uint8 input for speed |

**Performance Improvement:**

| Model | CPU | Edge TPU | Speedup |
|-------|-----|----------|---------|
| Herbify | 80-150ms | 10-15ms | ~10x |
| PlantDoc | 70-120ms | 5-12ms | ~10x |

Total inference time: **< 30ms** for both species ID and disease detection.

---

## Conclusion

This was an incredibly fun project, though not without its limitations. Budget constraints were significant‚Äîwhile support from my department helped immensely, I still lacked both sufficient funding and time. As a result, Herbot has no proper housing. I improvised the base and rotary stage using a car air freshener box, and attached the camera and gripper to the carriage with adhesive and electrical tape. It looks crude, but the cost-efficiency was unbeatable.

This project taught me a critical lesson about cost-effectiveness in engineering. Most university coursework exists in an unrealistic bubble where practical constraints like budget and robustness simply don't matter. But Herbot forced me to work within severe limitations‚Äîboth financial and computational‚Äîand that made me think deeply about engineering tradeoffs.

I realize now that this inexperience with real-world constraints was why my first drone project, Soltrone v1, failed. Back then, I naively thought I could just mount a flight controller, four propellers, and a Raspberry Pi together to create an autonomous drone. Reality hit hard with countless practical problems I hadn't anticipated. But the lessons from Herbot‚Äîunderstanding budget constraints, optimizing for limited resources, and making smart tradeoffs‚Äîwill be invaluable for Soltrone v2.
