---
layout: post
title: Hierarchical Relational Graph Learning for Autonomous Multiroboy Cooperative Navigation in Dynamic Environments 
subtitle: model structure study#2 
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/GPPO_HetGPPPO.png
share-img: /assets/img/path.jpg
tags: [MARL,GNN,HR]
author: Ting Wang, Xiao Du, Senior IEEE or etc.  
---

###Before diving into the paper 

이 논문의 robot navigation 문제에 대한 새로운 모델들을 제시하는 논문입니다. 산업 분야에서 이 문제는 굉장히 중요한 역할을 합니다. 산업 현장에서 로봇들이 장애물을 피하면서 서로 부딪히지 않으며, 주어진 임무를 수행해야 하기 때문입니다. 하지만 점점 더 기술이 발전함에 따라, 로봇의 복잡도가 높아졌습니다. 이에 따라 로봇들의 협력적인 관계 하의 navigation의 성능 역시 더 안정적이고, 뛰어난 성능을 요구하게 되었습니다. 현재 Deep 강화학습(DRL)의 잠재력은 대단하지만 현재 DRL은 확장성에 제한이 있다는 점과 새로운 작업이 주여졌을 때의 학습된 pocliy의 변환을 거의 고려하지 않는다는 문제점이 있습니다. 이에 저자는 두 개의 계층을 가진 graph network와 attention-based communication model를 가진 HDMR-Navi를 제안합니다. 또한 Maximum Entropy rl에 기반한 더 발전된 PPO 알고리즘(MEPPO)을 소개합니다. 

일단 기존의 방법들에서 어떤 문제들과 한계가 있었는지에 대해서 더 자세히 알아보겠습니다.<br> 
1. 기존의 방법들은 단순히 장애물의 영향에 대해 한 로봇과 한 장애물의 상호작용만을 고려한다는 점에서 전체 시스템에서의 적용에서 문제를 나타냅니다. 
2. 대부분의 방법들은 복잡한 환경에서 heterogeneous한 agents들에 대한 모델링을 제대로 해내지 못했습니다.
3. 대부분의 schemes은 성능이 나쁜 지식 전이성을 가지고 있어, 잘 학습된 pocliys이 있어도 다른 로봇들에게 다른 시나리오를 전달하는 부분에서 문제가 생깁니다. 
4. 대게 기존의 방법들은 centralized training decentralized execution(CTDE)를 사용합니다. 이것의 문제는 우선적으로 파라미터 공유를 합니다. 즉 다양한 상황에서도 같은 정책을 사용한다는 문제점이 발생합니다. 

->이러한 문제들로 기존의 방법들은 많은 수의 agents들을 다루는 시스템에서 제한되고 있습니다. 어떤 제한점이 있는지에 대해서는 아래에서 좀 더 살펴보겠습니다. 

###Robot Navigation 
실질적으로 논문에서는 multi robot navigation을 다룸으로, 이 문제에서 실제로 쓰이는 방법론들에 대해 설명한 후, HDMR-Navi에 대해 설명해보겠습니다. 

-Deep Reinforcement Learning-based schemes 
여기저기에서 많이 쓰이는 Multiagent Deep Deterministic Policy Gradient(MADDPG), Counterfactual Credit Assignment(COMA) 등의 알고리즘들이 사용되지만, 이 알고리즘들은 지식 전이성, 동적인 장애물에 대해서는 고려되지 못한다는 단점이 존재합니다. 물론 Mean Field가 도입되어 이웃 agent들과 평균 상태와 행동을 활용하여 의사결정을 하여 문제를 해결하려던 시도가 있었지만, 이웃들간 agents과의 다양한 영향은 고려하지 않았기에 한계가 존재했습니다. 그 후로도 이러한 한계를 극복하기 위한 시도들이 있었습니다. attentional communication model이나 soft attention 방식을 사용하는 decentralized policies를 학습시키는 actor-critic 알고리즘, 최근에는 imitation learning 과 multi agent RL을 결합하여 decentralized policy를 학습시키는 것까지 다양한 시도들이 있었습니다. 마지막에 언급한 연구도 결국은 centralized planning을 통해 학습된 전문적인 모델이 필요하다는 한계가 존재합니다. 
->결론: DRL based scheme들에서는 지식 전이성이나 CTDE 프레임워크를 사용한다는 한계가 존재하게 됩니다. 

### 여기서 잠깐 
두 알고리즘 모두 centralized training decentralized execution 방식을 사용하며 매우 간략하게 소개만 하겠습니다. 워낙 중요한 알고리즘들이라(이것도 추후에 포스팅하겠습니다!)
MADDPG: 이 전의 알고리즘들은 대부분 이산적인 행동 공간에 초점을 맞췄지만, 이 알고리즘은 특히 non-stationary(연속적인) 환경에서의 안정적인 학습을 가능하게 하였습니다. 
COMA: multi agent system에서 credit assignment 문제 해결이 큰 과제였는데, 이 알고리즘은 이 문제를 해결한 알고리즘입니다. 

### DRL + GNN-Based Schemes

최근 들어, Graph Neural Networks (GNN)이 로봇 네비게이션 분야에서 주목받고 있는 이유는, 비유클리드적 관계를 모델링하는 능력 덕분입니다. 단순히 유클리드 관계에서는 두 점 사이의 거리만을 고려했기에 많은 한계들이 있었지만, 비유클리드적 관계에서는 연결관계나 상호작용에 대한 패턴도 모델링할 수 있습니다. 또한 GNN은 여러 로봇들이 복잡한 비정형 공간에서 상호작용하며 이동할 때, 서로의 관계를 효과적으로 학습할 수 있도록 돕습니다. 특히 Hierarchical Attention-based Multi-agent Actor-Critic (HAMA), GPS (Graph Policy Search), 그리고 GAT (Graph Attention Networks)와 같은 여러 접근법이 제안되었습니다. 하지만 이러한 방법들도 여전히 비협력적이거나 움직이는 장애물에 대한 상호작용을 간과하거나 로봇들 간의 직접적인 통신을 고려하지 못하는 한계가 있습니다. 특히, GCN 기반의 연구는 단일 로봇에 중점을 두고 설계되어, 다중 로봇 간의 협력은 고려하지 못합니다.

결국, DRL과 GNN의 조합은 로봇 간의 비유클리드적 관계를 모델링하고, 로컬 관찰에 기반한 행동 결정을 가능하게 하지만, 아직은 협력적 네비게이션을 최적화하는 데 있어 다양한 장애 요소가 남아 있습니다.

### HRMR-Navi: 계층적 GNN 기반 네비게이션 모델

이 논문에서는 이러한 한계를 해결하고자 **Hierarchical Relational Graph Network (HRMR-Navi)**라는 새로운 DRL 기반 멀티로봇 네비게이션 모델을 제안합니다. HRMR-Navi의 핵심 아이디어는 다음과 같습니다:

1. **계층적 GNN 아키텍처**:
   - **에이전트 간 레이어 (Interagent Layer)**: GCN(Graph Convolutional Network)을 활용하여 로봇 간 관계를 효과적으로 모델링하고, 각 로봇이 자신과 이웃 로봇 및 장애물의 상태를 로컬 관찰 정보로부터 추론할 수 있게 합니다. 이 과정에서 로봇과 장애물의 관계는 다음 수식에 따라 모델링됩니다:

     $$
     R = \text{softmax}(A W_{\theta} W_{\phi}^T A^T)
     $$

     여기서:
     - \( A \): 로봇과 장애물의 임베딩 벡터를 포함한 **인접 행렬**입니다.
     - \( W_{\theta} \)와 \( W_{\phi} \): **학습 가능한 가중치 행렬**로, 로봇과 장애물 간 유사도를 계산하는 데 사용됩니다.
     - \( R \): 로봇 간 및 장애물과의 관계를 나타내는 **관계 행렬**로, 각 로봇이 이웃들과의 상호작용을 이해하는 데 사용됩니다.

   - **그룹 간 레이어 (Intergroup Layer)**: GAT(Graph Attention Network)를 사용하여 로봇 그룹과 장애물 그룹 간의 상호작용을 추론함으로써, 로봇이 특정 시점에 장애물 회피와 협력적 경로 최적화 중 어느 쪽에 더 집중해야 할지를 판단할 수 있도록 지원합니다. 로봇이 그룹 간 상호작용을 학습할 때는 다음과 같은 어텐션 가중치를 사용합니다:

     $$
     (\gamma_i^1, \gamma_i^2) \propto \exp \left( \left[ (h_i^1)^T W_k^T W_q W_{\theta} x_i, (h_i^2)^T W_k^T W_q W_{\theta} x_i \right] \right)
     $$

     여기서:
     - \( \gamma_i^1, \gamma_i^2 \): 로봇 \(i\)가 각 그룹(로봇과 장애물)에 얼마나 집중할지를 나타내는 **어텐션 가중치**입니다.
     - \( h_i^1, h_i^2 \): 로봇 그룹과 장애물 그룹 각각의 **노드 임베딩 벡터**입니다.
     - \( W_k \), \( W_q \), \( W_{\theta} \): 학습 가능한 가중치로, 로봇이 현재 상태에서 다른 로봇이나 장애물에 대한 집중도를 계산하는 데 사용됩니다.

2. **어텐션 기반 로봇 간 통신 모델**:
   - 각 로봇은 GAT 기반의 통신 모델을 통해 자신의 상태 및 주변 로봇의 정보를 통합하여, 전역 관점에서의 환경을 구성합니다. 이를 통해 각 로봇은 다중 홉 통신을 통해 효율적으로 정보를 교환하며, 협력적 네비게이션을 더욱 강화할 수 있습니다.

3. **Maximum Entropy PPO (MEPPO)**:
   - HRMR-Navi는 PPO(Proximal Policy Optimization)를 최대 엔트로피 강화 학습 프레임워크로 확장하여 로봇의 탐색 능력을 극대화합니다. MEPPO 알고리즘에서 사용되는 수식은 다음과 같습니다:

     $$
     \pi^* = \arg \max_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma \left( r(s_t, a_t, s_{t+1}) + \alpha H(\pi(\cdot|s_t)) \right) \right]
     $$

     여기서:
     - \( \pi^* \): 최적의 **정책**입니다.
     - \( \gamma \): **할인율**로, 미래의 보상에 대한 현재 가치의 가중치를 조절합니다.
     - \( r(s_t, a_t, s_{t+1}) \): 상태 \( s_t \)에서 행동 \( a_t \)를 취했을 때의 **보상**입니다.
     - \( H(\pi(\cdot|s_t)) \): 상태 \( s_t \)에서의 정책의 **엔트로피**로, 정책의 다양성을 보장하여 탐색을 강화하는 역할을 합니다.
     - \( \alpha \): 엔트로피의 중요성을 조정하는 **온도 파라미터**입니다.

---

### 구동 원리 

이제 HRMR-Navi가 어떻게 구동되는지 각 단계별로 설명하겠습니다.

1. **로컬 상태 관찰 (Input Processing)**:
   - 각 로봇은 주변 환경을 관찰하고, 자신과 이웃 로봇 및 이동 장애물의 위치, 속도, 방향 등의 정보를 수집하여 로컬 상태 벡터 s<sub>i</sub>를 구성합니다.

2. **에이전트 간 상호작용 모델링 (Interagent Interaction Modeling)**:
   - 각 로봇은 자신을 중심으로, 이웃 로봇과 장애물을 포함하는 그래프 구조를 생성하고 **관계 행렬 R**을 계산하여 이웃들과의 상호작용을 이해합니다. 각 로봇은 주변 객체들의 관계를 고려한 고차원 노드 임베딩 벡터 h<sub>i</sub>를 얻어냅니다.

3. **그룹 간 상호작용 추론 (Intergroup Interaction Reasoning)**:
   - 로봇은 로봇 그룹과 장애물 그룹 각각의 상호작용을 학습하여 어텐션 가중치 γ<sub>i</sub><sup>1</sup>, γ<sub>i</sub><sup>2</sup>을 계산하고, 협력적 경로 최적화와 장애물 회피의 중요도를 동적으로 조절합니다.

4. **어텐션 기반 로봇 간 통신 (Attention-Based Inter-Robot Communication)**:
   - 로봇은 이웃 로봇들과의 정보 교환을 통해 **어텐션 가중치 w<sub>ij</sub>**를 계산하고, 다중 홉 통신을 통해 글로벌 환경에 대한 이해를 구축합니다.

5. **최종 행동 결정 (Action Selection)**:
   - 각 로봇은 MEPPO를 통해 최종 행동을 결정하며, 엔트로피를 포함한 보상 함수를 통해 최적화하여 목표 지점에 빠르게 도달하면서도 충돌을 피합니다.

6. **실행 및 학습 (Execution and Learning)**:
   - 선택된 행동이 실행된 후 보상을 받아 정책을 업데이트하고, 로봇이 더욱 효율적인 경로를 학습하도록 합니다.

---

### 결론

HRMR-Navi는 계층적 그래프 네트워크와 어텐션 기반 통신을 통해 다중 로봇 네비게이션 문제를 효과적으로 해결하는 모델입니다. 실험 결과, 기존 모델들에 비해 충돌 없이 짧은 시간에 목표에 도달할 수 있었으며, 더 높은 확장성과 지식 전이 능력을 보여줍니다.

> **향후 연구 과제**로는 더욱 복잡한 환경에서의 실험 및 HRMR-Navi의 실제 시스템 적용 가능성에 대한 추가 연구가 필요할 것입니다.

### reference 
https://ieeexplore.ieee.org/document/10078380 

Ting Wang, Xiao Du, Mingsong Chen & Keqin Li(2023)Hierarchical Relational Graph Learning for
Autonomous Multirobot Cooperative Navigation
in Dynamic Environments