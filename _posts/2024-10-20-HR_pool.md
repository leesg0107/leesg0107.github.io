---
layout: post
title: Hierarchical Relational Graph Learning for Autonomous Multiroboy Cooperative Navigation in Dynamic Environments 
subtitle: model structure study#2 
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/GPPO_HetGPPPO.png
share-img: /assets/img/path.jpg
tags: [MARL,GNN,HR]
author: Ting Wang, Xiao Du, Senior IEEE or etc.  
---

###Before diving into the paper 

이 논문의 robot navigation 문제에 대한 새로운 모델들을 제시하는 논문입니다. 산업 분야에서 이 문제는 굉장히 중요한 역할을 합니다. 산업 현장에서 로봇들이 장애물을 피하면서 서로 부딪히지 않으며, 주어진 임무를 수행해야 하기 때문입니다. 하지만 점점 더 기술이 발전함에 따라, 로봇의 복잡도가 높아졌습니다. 이에 따라 로봇들의 협력적인 관계 하의 navigation의 성능 역시 더 안정적이고, 뛰어난 성능을 요구하게 되었습니다. 현재 Deep 강화학습(DRL)의 잠재력은 대단하지만 현재 DRL은 확장성에 제한이 있다는 점과 새로운 작업이 주여졌을 때의 학습된 pocliy의 변환을 거의 고려하지 않는다는 문제점이 있습니다. 이에 저자는 두 개의 계층을 가진 graph network와 attention-based communication model를 가진 HDMR-Navi를 제안합니다. 또한 Maximum Entropy rl에 기반한 더 발전된 PPO 알고리즘(MEPPO)을 소개합니다. 

일단 기존의 방법들에서 어떤 문제들과 한계가 있었는지에 대해서 더 자세히 알아보겠습니다.<br> 
1. 기존의 방법들은 단순히 장애물의 영향에 대해 한 로봇과 한 장애물의 상호작용만을 고려한다는 점에서 전체 시스템에서의 적용에서 문제를 나타냅니다. 
2. 대부분의 방법들은 복잡한 환경에서 heterogeneous한 agents들에 대한 모델링을 제대로 해내지 못했습니다.
3. 대부분의 schemes은 성능이 나쁜 지식 전이성을 가지고 있어, 잘 학습된 pocliys이 있어도 다른 로봇들에게 다른 시나리오를 전달하는 부분에서 문제가 생깁니다. 
4. 대게 기존의 방법들은 centralized training decentralized execution(CTDE)를 사용합니다. 이것의 문제는 우선적으로 파라미터 공유를 합니다. 즉 다양한 상황에서도 같은 정책을 사용한다는 문제점이 발생합니다. 

->이러한 문제들로 기존의 방법들은 많은 수의 agents들을 다루는 시스템에서 제한되고 있습니다.  

###Robot Navigation 
실질적으로 논문에서는 multi robot navigation을 다룸으로, 이 문제에서 실제로 쓰이는 방법론들에 대해 설명한 후, HDMR-Navi에 대해 설명해보겠습니다. 

-Deep Reinforcement Learning-based schemes 
여기저기에서 많이 쓰이는 Multiagent Deep Deterministic Policy Gradient(MADDPG), Counterfactual Credit Assignment(COMA) 등의 알고리즘들이 사용되지만, 이 알고리즘들은 지식 전이성, 동적인 장애물에 대해서는 고려되지 못한다는 단점이 존재합니다. 물론 Mean Field가 도입되어 이웃 agent들과 평균 상태와 행동을 활용하여 의사결정을 하여 문제를 해결하려던 시도가 있었지만, 이웃들간 agents과의 다양한 영향은 고려하지 않았기에 한계가 존재했습니다. 그 후로도 이러한 한계를 극복하기 위한 시도들이 있었습니다. attentional communication model이나 soft attention 방식을 사용하는 decentralized policies를 학습시키는 actor-critic 알고리즘, 최근에는 imitation learning 과 multi agent RL을 결합하여 decentralized policy를 학습시키는 것까지 다양한 시도들이 있었습니다. 마지막에 언급한 연구도 결국은 centralized planning을 통해 학습된 전문적인 모델이 필요하다는 한계가 존재합니다. 
->결론: DRL based scheme들에서는 지식 전이성이나 CTDE 프레임워크를 사용한다는 한계가 존재하게 됩니다. 

###여기서 잠깐 
두 알고리즘 모두 centralized training decentralized execution 방식을 사용하며 매우 간략하게 소개만 하겠습니다. 워낙 중요한 알고리즘들이라(이것도 추후에 포스팅하겠습니다!)
MADDPG: 이 전의 알고리즘들은 대부분 이산적인 행동 공간에 초점을 맞췄지만, 이 알고리즘은 특히 non-stationary(연속적인) 환경에서의 안정적인 학습을 가능하게 하였습니다. 
COMA: multi agent system에서 credit assignment 문제 해결이 큰 과제였는데, 이 알고리즘은 이 문제를 해결한 알고리즘입니다. 

-DRL+GNN-based schemes 
